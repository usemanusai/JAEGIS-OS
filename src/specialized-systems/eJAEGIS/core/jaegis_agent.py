"""
e.J.A.E.G.I.S. (Enhanced Multi-agent Architecture & Dependency Specialist)
Core Agent Implementation

A perpetual, active background monitor for project codebases that identifies,
analyzes, and reports on dependencies affected by code modifications.
"""

import os
import json
import asyncio
import logging
from pathlib import Path
from typing import Dict, List, Set, Optional, Any
from dataclasses import dataclass, asdict
from datetime import datetime
import hashlib

from watchdog.observers import Observer
from watchdog.events import FileSystemEventHandler
from neo4j import GraphDatabase
import ast
import re
from concurrent.futures import ThreadPoolExecutor
import aiohttp
import yaml

# Configure logging
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger('E-JAEGIS')

@dataclass
class CodeComponent:
    """Represents a code component in the knowledge graph"""
    file_path: str
    component_type: str  # 'function', 'class', 'module', 'variable'
    name: str
    line_start: int
    line_end: int
    dependencies: List[str]
    dependents: List[str]
    hash_signature: str
    last_modified: datetime

@dataclass
class OwnershipEntry:
    """Represents an ownership entry in the ledger"""
    pattern: str  # File pattern or specific path
    owner_type: str  # 'agent' or 'human'
    owner_id: str  # Agent name or human identifier
    contact_method: str  # How to notify this owner
    priority: int  # Priority level for notifications

@dataclass
class ImpactTask:
    """Represents a task generated by impact analysis"""
    task_id: str
    affected_component: str
    change_description: str
    impact_level: str  # 'low', 'medium', 'high', 'critical'
    owner: OwnershipEntry
    suggested_actions: List[str]
    created_at: datetime

class CodebaseKnowledgeGraph:
    """Maintains real-time model of project architecture and dependencies"""
    
    def __init__(self, project_root: Path, neo4j_uri: str = "bolt://localhost:7687"):
        self.project_root = project_root
        self.neo4j_uri = neo4j_uri
        self.driver = None
        self.components: Dict[str, CodeComponent] = {}
        
    async def initialize(self):
        """Initialize the knowledge graph"""
        try:
            self.driver = GraphDatabase.driver(self.neo4j_uri)
            await self._create_constraints()
            logger.info("Knowledge graph initialized successfully")
        except Exception as e:
            logger.warning(f"Neo4j not available, using in-memory graph: {e}")
            self.driver = None
    
    async def _create_constraints(self):
        """Create Neo4j constraints and indexes"""
        if not self.driver:
            return
            
        with self.driver.session() as session:
            # Create constraints
            session.run("CREATE CONSTRAINT IF NOT EXISTS FOR (c:Component) REQUIRE c.id IS UNIQUE")
            session.run("CREATE INDEX IF NOT EXISTS FOR (c:Component) ON (c.file_path)")
            session.run("CREATE INDEX IF NOT EXISTS FOR (c:Component) ON (c.component_type)")
    
    async def build_initial_graph(self):
        """Build initial knowledge graph from project scan"""
        logger.info("Building initial knowledge graph...")
        
        # Scan all source files
        source_files = self._find_source_files()
        
        with ThreadPoolExecutor(max_workers=4) as executor:
            tasks = [
                asyncio.get_event_loop().run_in_executor(
                    executor, self._analyze_file, file_path
                )
                for file_path in source_files
            ]
            
            results = await asyncio.gather(*tasks, return_exceptions=True)
            
        # Process results
        for result in results:
            if isinstance(result, Exception):
                logger.error(f"Error analyzing file: {result}")
            elif result:
                await self._add_components_to_graph(result)
        
        logger.info(f"Knowledge graph built with {len(self.components)} components")
    
    def _find_source_files(self) -> List[Path]:
        """Find all source code files in the project"""
        extensions = {'.py', '.js', '.ts', '.java', '.cpp', '.c', '.h', '.go', '.rs', '.rb'}
        source_files = []
        
        for root, dirs, files in os.walk(self.project_root):
            # Skip common non-source directories
            dirs[:] = [d for d in dirs if not d.startswith('.') and d not in {'node_modules', '__pycache__', 'venv', 'env'}]
            
            for file in files:
                if Path(file).suffix in extensions:
                    source_files.append(Path(root) / file)
        
        return source_files
    
    def _analyze_file(self, file_path: Path) -> List[CodeComponent]:
        """Analyze a single file and extract components"""
        try:
            with open(file_path, 'r', encoding='utf-8') as f:
                content = f.read()
            
            # Generate file hash
            file_hash = hashlib.md5(content.encode()).hexdigest()
            
            # Language-specific analysis
            if file_path.suffix == '.py':
                return self._analyze_python_file(file_path, content, file_hash)
            elif file_path.suffix in {'.js', '.ts'}:
                return self._analyze_javascript_file(file_path, content, file_hash)
            else:
                # Generic analysis for other languages
                return self._analyze_generic_file(file_path, content, file_hash)
                
        except Exception as e:
            logger.error(f"Error analyzing {file_path}: {e}")
            return []
    
    def _analyze_python_file(self, file_path: Path, content: str, file_hash: str) -> List[CodeComponent]:
        """Analyze Python file using AST"""
        components = []
        
        try:
            tree = ast.parse(content)
            
            for node in ast.walk(tree):
                if isinstance(node, ast.FunctionDef):
                    component = CodeComponent(
                        file_path=str(file_path.relative_to(self.project_root)),
                        component_type='function',
                        name=node.name,
                        line_start=node.lineno,
                        line_end=node.end_lineno or node.lineno,
                        dependencies=self._extract_python_dependencies(node),
                        dependents=[],
                        hash_signature=file_hash,
                        last_modified=datetime.now()
                    )
                    components.append(component)
                    
                elif isinstance(node, ast.ClassDef):
                    component = CodeComponent(
                        file_path=str(file_path.relative_to(self.project_root)),
                        component_type='class',
                        name=node.name,
                        line_start=node.lineno,
                        line_end=node.end_lineno or node.lineno,
                        dependencies=self._extract_python_dependencies(node),
                        dependents=[],
                        hash_signature=file_hash,
                        last_modified=datetime.now()
                    )
                    components.append(component)
                    
        except SyntaxError as e:
            logger.warning(f"Syntax error in {file_path}: {e}")
        
        return components
    
    def _extract_python_dependencies(self, node: ast.AST) -> List[str]:
        """Extract dependencies from Python AST node"""
        dependencies = []
        
        for child in ast.walk(node):
            if isinstance(child, ast.Import):
                for alias in child.names:
                    dependencies.append(alias.name)
            elif isinstance(child, ast.ImportFrom):
                if child.module:
                    dependencies.append(child.module)
            elif isinstance(child, ast.Name):
                dependencies.append(child.id)
        
        return list(set(dependencies))  # Remove duplicates
    
    def _analyze_javascript_file(self, file_path: Path, content: str, file_hash: str) -> List[CodeComponent]:
        """Analyze JavaScript/TypeScript file using regex patterns"""
        components = []
        lines = content.split('\n')
        
        # Function declarations
        func_pattern = r'(?:function\s+(\w+)|const\s+(\w+)\s*=\s*(?:async\s+)?(?:\([^)]*\)\s*=>|\([^)]*\)\s*{)|(\w+)\s*:\s*(?:async\s+)?(?:function|\([^)]*\)\s*=>))'
        
        for i, line in enumerate(lines):
            match = re.search(func_pattern, line)
            if match:
                func_name = match.group(1) or match.group(2) or match.group(3)
                if func_name:
                    component = CodeComponent(
                        file_path=str(file_path.relative_to(self.project_root)),
                        component_type='function',
                        name=func_name,
                        line_start=i + 1,
                        line_end=i + 1,  # Simplified - would need proper parsing for end
                        dependencies=self._extract_js_dependencies(content),
                        dependents=[],
                        hash_signature=file_hash,
                        last_modified=datetime.now()
                    )
                    components.append(component)
        
        return components
    
    def _extract_js_dependencies(self, content: str) -> List[str]:
        """Extract dependencies from JavaScript content"""
        dependencies = []
        
        # Import statements
        import_patterns = [
            r'import\s+.*?\s+from\s+[\'"]([^\'"]+)[\'"]',
            r'require\s*\(\s*[\'"]([^\'"]+)[\'"]\s*\)',
            r'import\s*\(\s*[\'"]([^\'"]+)[\'"]\s*\)'
        ]
        
        for pattern in import_patterns:
            matches = re.findall(pattern, content)
            dependencies.extend(matches)
        
        return list(set(dependencies))
    
    def _analyze_generic_file(self, file_path: Path, content: str, file_hash: str) -> List[CodeComponent]:
        """Generic analysis for unsupported file types"""
        # Basic analysis - just track the file as a component
        component = CodeComponent(
            file_path=str(file_path.relative_to(self.project_root)),
            component_type='file',
            name=file_path.name,
            line_start=1,
            line_end=len(content.split('\n')),
            dependencies=[],
            dependents=[],
            hash_signature=file_hash,
            last_modified=datetime.now()
        )
        return [component]
    
    async def _add_components_to_graph(self, components: List[CodeComponent]):
        """Add components to the knowledge graph"""
        for component in components:
            component_id = f"{component.file_path}:{component.name}"
            self.components[component_id] = component
            
            if self.driver:
                await self._add_to_neo4j(component)
    
    async def _add_to_neo4j(self, component: CodeComponent):
        """Add component to Neo4j database"""
        if not self.driver:
            return
            
        with self.driver.session() as session:
            session.run("""
                MERGE (c:Component {id: $id})
                SET c.file_path = $file_path,
                    c.component_type = $component_type,
                    c.name = $name,
                    c.line_start = $line_start,
                    c.line_end = $line_end,
                    c.hash_signature = $hash_signature,
                    c.last_modified = $last_modified
            """, 
                id=f"{component.file_path}:{component.name}",
                file_path=component.file_path,
                component_type=component.component_type,
                name=component.name,
                line_start=component.line_start,
                line_end=component.line_end,
                hash_signature=component.hash_signature,
                last_modified=component.last_modified.isoformat()
            )
    
    async def find_dependents(self, component_id: str) -> List[str]:
        """Find all components that depend on the given component"""
        if self.driver:
            return await self._find_dependents_neo4j(component_id)
        else:
            return self._find_dependents_memory(component_id)
    
    async def _find_dependents_neo4j(self, component_id: str) -> List[str]:
        """Find dependents using Neo4j graph traversal"""
        with self.driver.session() as session:
            result = session.run("""
                MATCH (c:Component {id: $component_id})<-[:DEPENDS_ON*1..3]-(dependent)
                RETURN dependent.id as dependent_id
            """, component_id=component_id)
            
            return [record["dependent_id"] for record in result]
    
    def _find_dependents_memory(self, component_id: str) -> List[str]:
        """Find dependents using in-memory graph traversal"""
        dependents = []
        
        if component_id not in self.components:
            return dependents
        
        target_component = self.components[component_id]
        
        # Simple dependency matching based on imports and references
        for comp_id, component in self.components.items():
            if comp_id == component_id:
                continue
                
            # Check if this component depends on the target
            if (target_component.name in component.dependencies or
                target_component.file_path in component.dependencies):
                dependents.append(comp_id)
        
        return dependents
    
    async def update_component(self, file_path: str, new_content: str):
        """Update component information after file change"""
        file_path_obj = Path(file_path)
        
        # Re-analyze the changed file
        new_components = self._analyze_file(file_path_obj)
        
        # Update the graph
        await self._add_components_to_graph(new_components)
        
        logger.info(f"Updated knowledge graph for {file_path}")

class OwnershipLedger:
    """Manages the ownership ledger for code components"""
    
    def __init__(self, project_root: Path):
        self.project_root = project_root
        self.ledger_path = project_root / "OWNERSHIP_LEDGER.json"
        self.ownership_entries: List[OwnershipEntry] = []
    
    async def initialize(self):
        """Initialize the ownership ledger"""
        if self.ledger_path.exists():
            await self.load_ledger()
        else:
            await self.create_default_ledger()
    
    async def load_ledger(self):
        """Load ownership ledger from file"""
        try:
            with open(self.ledger_path, 'r') as f:
                data = json.load(f)
            
            self.ownership_entries = [
                OwnershipEntry(**entry) for entry in data.get('ownership_entries', [])
            ]
            
            logger.info(f"Loaded {len(self.ownership_entries)} ownership entries")
            
        except Exception as e:
            logger.error(f"Error loading ownership ledger: {e}")
            await self.create_default_ledger()
    
    async def create_default_ledger(self):
        """Create a default ownership ledger template"""
        default_entries = [
            OwnershipEntry(
                pattern="src/api/**",
                owner_type="agent",
                owner_id="API-Agent",
                contact_method="ide_notification",
                priority=1
            ),
            OwnershipEntry(
                pattern="src/core/**",
                owner_type="agent", 
                owner_id="Core-Architecture-Agent",
                contact_method="ide_notification",
                priority=1
            ),
            OwnershipEntry(
                pattern="tests/**",
                owner_type="agent",
                owner_id="QA-Agent",
                contact_method="ide_notification",
                priority=2
            ),
            OwnershipEntry(
                pattern="**",
                owner_type="human",
                owner_id="dev_team_lead",
                contact_method="slack",
                priority=3
            )
        ]
        
        self.ownership_entries = default_entries
        await self.save_ledger()
        
        logger.info("Created default ownership ledger")
    
    async def save_ledger(self):
        """Save ownership ledger to file"""
        data = {
            "version": "1.0",
            "last_updated": datetime.now().isoformat(),
            "ownership_entries": [asdict(entry) for entry in self.ownership_entries]
        }
        
        with open(self.ledger_path, 'w') as f:
            json.dump(data, f, indent=2)
    
    def find_owner(self, file_path: str) -> Optional[OwnershipEntry]:
        """Find the owner for a given file path"""
        import fnmatch
        
        # Sort by priority (lower number = higher priority)
        sorted_entries = sorted(self.ownership_entries, key=lambda x: x.priority)
        
        for entry in sorted_entries:
            if fnmatch.fnmatch(file_path, entry.pattern):
                return entry
        
        return None

class ActiveMonitoringService:
    """Dual-mode service for monitoring file changes"""

    def __init__(self, project_root: Path, eJaegis_agent):
        self.project_root = project_root
        self.eJaegis_agent = eJaegis_agent
        self.observer = None
        self.is_monitoring = False

    async def start_local_monitoring(self):
        """Start local file system monitoring"""
        if self.is_monitoring:
            return

        event_handler = E-JAEGISFileHandler(self.eJaegis_agent)
        self.observer = Observer()
        self.observer.schedule(event_handler, str(self.project_root), recursive=True)
        self.observer.start()
        self.is_monitoring = True

        logger.info(f"Started local monitoring for {self.project_root}")

    async def stop_monitoring(self):
        """Stop file system monitoring"""
        if self.observer and self.is_monitoring:
            self.observer.stop()
            self.observer.join()
            self.is_monitoring = False
            logger.info("Stopped local monitoring")

    async def setup_webhook_monitoring(self, webhook_url: str, secret: str):
        """Setup webhook monitoring for remote repositories"""
        # This would integrate with GitHub/GitLab webhooks
        logger.info(f"Webhook monitoring setup for {webhook_url}")

class E-JAEGISFileHandler(FileSystemEventHandler):
    """File system event handler for e.J.A.E.G.I.S."""

    def __init__(self, eJaegis_agent):
        self.eJaegis_agent = eJaegis_agent
        self.debounce_delay = 1.0  # Seconds to wait before processing
        self.pending_changes = {}

    def on_modified(self, event):
        if event.is_directory:
            return

        file_path = Path(event.src_path)

        # Filter for source code files
        if file_path.suffix in {'.py', '.js', '.ts', '.java', '.cpp', '.c', '.h', '.go', '.rs', '.rb'}:
            # Debounce rapid file changes
            self.pending_changes[str(file_path)] = datetime.now()

            # Schedule processing after debounce delay
            asyncio.create_task(self._process_change_after_delay(str(file_path)))

    async def _process_change_after_delay(self, file_path: str):
        """Process file change after debounce delay"""
        await asyncio.sleep(self.debounce_delay)

        # Check if this is still the latest change for this file
        if (file_path in self.pending_changes and
            (datetime.now() - self.pending_changes[file_path]).total_seconds() >= self.debounce_delay):

            await self.eJaegis_agent.process_file_change(file_path)
            del self.pending_changes[file_path]

class ImpactAnalysisEngine:
    """Core logic for analyzing the impact of code changes"""

    def __init__(self, knowledge_graph: CodebaseKnowledgeGraph, ownership_ledger: OwnershipLedger):
        self.knowledge_graph = knowledge_graph
        self.ownership_ledger = ownership_ledger
        self.llm_client = None  # Will be initialized with API client

    async def analyze_impact(self, file_path: str, change_diff: str = None) -> List[ImpactTask]:
        """Analyze the impact of a file change"""
        tasks = []

        # Find all components in the changed file
        file_components = [
            comp_id for comp_id, comp in self.knowledge_graph.components.items()
            if comp.file_path == file_path
        ]

        # For each component, find its dependents
        for component_id in file_components:
            dependents = await self.knowledge_graph.find_dependents(component_id)

            for dependent_id in dependents:
                dependent_comp = self.knowledge_graph.components.get(dependent_id)
                if not dependent_comp:
                    continue

                # Find owner of the dependent component
                owner = self.ownership_ledger.find_owner(dependent_comp.file_path)
                if not owner:
                    continue

                # Analyze impact level
                impact_level = await self._assess_impact_level(component_id, dependent_id, change_diff)

                # Generate suggested actions
                suggested_actions = await self._generate_suggested_actions(
                    component_id, dependent_id, impact_level, change_diff
                )

                # Create impact task
                task = ImpactTask(
                    task_id=f"impact_{datetime.now().strftime('%Y%m%d_%H%M%S')}_{len(tasks)}",
                    affected_component=dependent_id,
                    change_description=f"Change in {component_id} affects {dependent_id}",
                    impact_level=impact_level,
                    owner=owner,
                    suggested_actions=suggested_actions,
                    created_at=datetime.now()
                )

                tasks.append(task)

        return tasks

    async def _assess_impact_level(self, changed_component: str, affected_component: str, change_diff: str = None) -> str:
        """Assess the impact level of a change"""
        # Simple heuristic-based assessment
        # In production, this would use LLM analysis

        changed_comp = self.knowledge_graph.components.get(changed_component)
        affected_comp = self.knowledge_graph.components.get(affected_component)

        if not changed_comp or not affected_comp:
            return "medium"

        # API changes are typically high impact
        if "api" in changed_comp.file_path.lower():
            return "high"

        # Core/shared components are high impact
        if "core" in changed_comp.file_path.lower() or "shared" in changed_comp.file_path.lower():
            return "high"

        # Test files are typically low impact
        if "test" in affected_comp.file_path.lower():
            return "low"

        return "medium"

    async def _generate_suggested_actions(self, changed_component: str, affected_component: str,
                                        impact_level: str, change_diff: str = None) -> List[str]:
        """Generate suggested actions for handling the impact"""
        actions = []

        changed_comp = self.knowledge_graph.components.get(changed_component)
        affected_comp = self.knowledge_graph.components.get(affected_component)

        if not changed_comp or not affected_comp:
            return ["Review the change and update dependent code as needed"]

        if impact_level == "high":
            actions.extend([
                f"Review changes in {changed_comp.file_path}",
                f"Update {affected_comp.file_path} to handle interface changes",
                "Run integration tests",
                "Update documentation if needed"
            ])
        elif impact_level == "medium":
            actions.extend([
                f"Review changes in {changed_comp.file_path}",
                f"Verify {affected_comp.file_path} still works correctly",
                "Run relevant tests"
            ])
        else:  # low impact
            actions.extend([
                f"Verify {affected_comp.file_path} is not affected by changes",
                "Run basic tests"
            ])

        return actions

class TaskDispatchRouter:
    """Central hub for notifications and task routing"""

    def __init__(self, project_root: Path):
        self.project_root = project_root
        self.notification_handlers = {}
        self.task_queue = asyncio.Queue()

    async def initialize(self):
        """Initialize the task dispatch router"""
        # Register notification handlers
        self.notification_handlers = {
            "ide_notification": self._send_ide_notification,
            "slack": self._send_slack_notification,
            "email": self._send_email_notification,
            "webhook": self._send_webhook_notification
        }

        # Start task processing
        asyncio.create_task(self._process_task_queue())

    async def dispatch_task(self, task: ImpactTask):
        """Dispatch a task to the appropriate handler"""
        await self.task_queue.put(task)

    async def _process_task_queue(self):
        """Process tasks from the queue"""
        while True:
            try:
                task = await self.task_queue.get()
                await self._handle_task(task)
                self.task_queue.task_done()
            except Exception as e:
                logger.error(f"Error processing task: {e}")

    async def _handle_task(self, task: ImpactTask):
        """Handle a single task"""
        handler = self.notification_handlers.get(task.owner.contact_method)
        if handler:
            await handler(task)
        else:
            logger.warning(f"No handler for contact method: {task.owner.contact_method}")

    async def _send_ide_notification(self, task: ImpactTask):
        """Send notification to IDE"""
        # This would integrate with Language Server Protocol
        notification = {
            "method": "window_showMessageparams": {
                "type": 2,  # Warning
                "message": f"e.J.A.E.G.I.S. Alert: {task.change_description}"
            }
        }

        # Write to IDE notification file
        notification_file = self.project_root / ".eJaegis" / "ide_notifications.json"
        notification_file.parent.mkdir(exist_ok=True)

        notifications = []
        if notification_file.exists():
            with open(notification_file, 'r') as f:
                notifications = json.load(f)

        notifications.append({
            "timestamp": task.created_at.isoformat(),
            "task_id": task.task_id,
            "message": task.change_description,
            "impact_level": task.impact_level,
            "suggested_actions": task.suggested_actions
        })

        with open(notification_file, 'w') as f:
            json.dump(notifications[-10:], f, indent=2)  # Keep last 10 notifications

        logger.info(f"IDE notification sent for task {task.task_id}")

    async def _send_slack_notification(self, task: ImpactTask):
        """Send notification to Slack"""
        # This would integrate with Slack API
        logger.info(f"Slack notification sent for task {task.task_id}")

    async def _send_email_notification(self, task: ImpactTask):
        """Send email notification"""
        # This would integrate with email service
        logger.info(f"Email notification sent for task {task.task_id}")

    async def _send_webhook_notification(self, task: ImpactTask):
        """Send webhook notification"""
        # This would send HTTP POST to configured webhook
        logger.info(f"Webhook notification sent for task {task.task_id}")

class E-JAEGISAgent:
    """Main e.J.A.E.G.I.S. Agent orchestrating all components"""

    def __init__(self, project_root: Path):
        self.project_root = project_root
        self.eJaegis_dir = project_root / ".eJaegis"

        # Initialize components
        self.knowledge_graph = CodebaseKnowledgeGraph(project_root)
        self.ownership_ledger = OwnershipLedger(project_root)
        self.monitoring_service = ActiveMonitoringService(project_root, self)
        self.impact_engine = ImpactAnalysisEngine(self.knowledge_graph, self.ownership_ledger)
        self.task_router = TaskDispatchRouter(project_root)

        self.is_initialized = False

    async def initialize(self):
        """Initialize the e.J.A.E.G.I.S. agent"""
        if self.is_initialized:
            return

        logger.info("Initializing e.J.A.E.G.I.S. Agent...")

        # Create .eJaegis directory
        self.eJaegis_dir.mkdir(exist_ok=True)

        # Initialize all components
        await self.knowledge_graph.initialize()
        await self.ownership_ledger.initialize()
        await self.task_router.initialize()

        # Build initial knowledge graph
        await self.knowledge_graph.build_initial_graph()

        # Start monitoring
        await self.monitoring_service.start_local_monitoring()

        self.is_initialized = True
        logger.info("e.J.A.E.G.I.S. Agent initialized successfully")

    async def process_file_change(self, file_path: str):
        """Process a file change event"""
        try:
            logger.info(f"Processing file change: {file_path}")

            # Update knowledge graph
            with open(file_path, 'r', encoding='utf-8') as f:
                new_content = f.read()

            await self.knowledge_graph.update_component(file_path, new_content)

            # Analyze impact
            impact_tasks = await self.impact_engine.analyze_impact(file_path)

            # Dispatch tasks
            for task in impact_tasks:
                await self.task_router.dispatch_task(task)

            logger.info(f"Generated {len(impact_tasks)} impact tasks for {file_path}")

        except Exception as e:
            logger.error(f"Error processing file change {file_path}: {e}")

    async def shutdown(self):
        """Shutdown the e.J.A.E.G.I.S. agent"""
        logger.info("Shutting down e.J.A.E.G.I.S. Agent...")

        await self.monitoring_service.stop_monitoring()

        if self.knowledge_graph.driver:
            self.knowledge_graph.driver.close()

        logger.info("e.J.A.E.G.I.S. Agent shutdown complete")
